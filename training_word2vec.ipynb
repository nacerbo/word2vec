{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f0710e6d",
   "metadata": {},
   "source": [
    "# What is Word2Vec Model\n",
    "\n",
    "Word2Vec is a popular algorithm used for natural language processing (NLP) tasks, specifically for learning word embeddings, which are dense numerical representations of words. It was developed by Tomas Mikolov and his colleagues at Google in 2013.\n",
    "\n",
    "Word2Vec is based on the idea that words with similar meanings tend to appear in similar contexts. The algorithm takes a large corpus of text as input and learns to represent words in a continuous vector space, where words with similar meanings are close. The resulting word vectors capture semantic relationships between terms, such as analogies and similarities.\n",
    "\n",
    "Two main architectures are used in Word2Vec: Continuous Bag-of-Words (CBOW) and Skip-gram. In CBOW, the algorithm predicts the target word based on its surrounding context words. In contrast, Skip-gram predicts the context words given a target word. Both architectures use a shallow neural network with a single hidden layer to learn the word embeddings.\n",
    "\n",
    "Once trained, the word vectors can be used for various NLP tasks. For example, they can be used to compute the similarity between words or find semantically related words. The word vectors can also be used as input features for downstream machine learning models in tasks like text classification, sentiment analysis, machine translation, and more.\n",
    "\n",
    "Word2Vec has been widely adopted and has significantly impacted the field of NLP. Its ability to capture semantic relationships between words in an unsupervised manner has made it a valuable tool for many language-related applications."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cc6153eb",
   "metadata": {},
   "source": [
    "# Training Word2Vec model\n",
    "Trained on personal over 270k english only articles (dataset size 1.22GB)\n",
    "The articles data was scrapped off of multiple englush news and blog posts, for reasearch personal purposes only.\n",
    "\n",
    "you can use wikipedia dataset of 20GB from this link\n",
    "https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d66e0482",
   "metadata": {},
   "source": [
    "### Installing libraries needed\n",
    "before we can train the model we need to install the libraries needed like (nltk, pandas, gensim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c5341bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy\n",
      "  Downloading numpy-1.25.0-cp39-cp39-macosx_11_0_arm64.whl (14.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting pandas\n",
      "  Downloading pandas-2.0.2-cp39-cp39-macosx_11_0_arm64.whl (10.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hCollecting nltk\n",
      "  Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "Collecting gensim\n",
      "  Downloading gensim-4.3.1-cp39-cp39-macosx_11_0_arm64.whl (24.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.0/24.0 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in ./.env/lib/python3.9/site-packages (from pandas) (2.8.2)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2023.3-py2.py3-none-any.whl (502 kB)\n",
      "Collecting tzdata>=2022.1 (from pandas)\n",
      "  Using cached tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "Collecting click (from nltk)\n",
      "  Using cached click-8.1.3-py3-none-any.whl (96 kB)\n",
      "Collecting joblib (from nltk)\n",
      "  Using cached joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2023.6.3-cp39-cp39-macosx_11_0_arm64.whl (288 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.0/289.0 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tqdm (from nltk)\n",
      "  Using cached tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
      "Collecting scipy>=1.7.0 (from gensim)\n",
      "  Downloading scipy-1.10.1-cp39-cp39-macosx_12_0_arm64.whl (28.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m28.9/28.9 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting smart-open>=1.8.1 (from gensim)\n",
      "  Downloading smart_open-6.3.0-py3-none-any.whl (56 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in ./.env/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Installing collected packages: pytz, tzdata, tqdm, smart-open, regex, numpy, joblib, click, scipy, pandas, nltk, gensim\n",
      "Successfully installed click-8.1.3 gensim-4.3.1 joblib-1.2.0 nltk-3.8.1 numpy-1.25.0 pandas-2.0.2 pytz-2023.3 regex-2023.6.3 scipy-1.10.1 smart-open-6.3.0 tqdm-4.65.0 tzdata-2023.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install numpy pandas nltk gensim"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cbcf2bfd",
   "metadata": {},
   "source": [
    "### importing the libraries\n",
    "after installing our libraries now we can implement word2vec by first importing all the necessery libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02311807-59c0-4cb6-b9c7-bd245cd0de19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import string\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "87345579",
   "metadata": {},
   "source": [
    "### Preprocessing the text\n",
    "Before we load the text to our model to train we will need to preprocess it by turning all characters to lowercase, delete any numerical characters, special characters, and any white space at the start or end.\n",
    "and generate the tokens (separating words and turning it to list of words instead of lines and paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38c7883a-1cb4-45dc-9dad-ac5fde27764c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(a_string):\n",
    "    a_string = a_string.lower() # make all characters lower case\n",
    "    a_string = re.sub(r'\\d+', '', a_string) # delete any numbers in the text\n",
    "    a_string = a_string.translate(str.maketrans('', '', string.punctuation)) # remove [!”#$%&’()*+,-./:;<=>?@[\\]^_`{|}~]\n",
    "    a_string = a_string.strip() # delete whitespace\n",
    "    return word_tokenize(a_string)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0d88da6b",
   "metadata": {},
   "source": [
    "Defining the path to our text dataset that we prepared by scrapping online sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64b1e4c9-c7a9-459e-9c30-325e467787b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = \"articles.txt\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3ab36e08",
   "metadata": {},
   "source": [
    "generate a list of lines from the txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2cf154a2-6b5e-483d-a620-09c63f945846",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = []\n",
    "\n",
    "with open(path_to_file) as f:\n",
    "    lines = f.read().splitlines()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f074e4fd",
   "metadata": {},
   "source": [
    "as we can see we have over 7 million lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e131ade8-23a8-456c-acfc-90c432585f17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7152556"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb449db6-3f37-4796-ad44-95463ca99664",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Nancy Pelosi receives a standing ovation from audience at Clive Davis’ party',\n",
       " \"Nancy Pelosi recently attended Clive Davis' annual pre-Grammys party, where she received a standing ovation.\",\n",
       " 'The event took place on Saturday, February 9, 2019, at the Beverly Hilton Hotel, in Beverly Hills, and attracted several big names from the music industry.',\n",
       " 'Speaker of the House Nancy Pelosi | Photo: Getty Images',\n",
       " 'RECOGNIZED BY THE OTHER GUESTS',\n",
       " \"After hearing Davis' words, everyone in the audience stood up to honor Pelosi, who also rose from her chair to express her gratitude towards the other party guests.\",\n",
       " 'A VERY EXCLUSIVE GATHERING',\n",
       " \"Davis' pre-Grammys party is a much-anticipated event by those who are lucky enough to be invited and the guest list is usually filled with the names of famous performers such as Pharrell, Charlie Puth, Dionne Warwick and Beck.\",\n",
       " \"But one doesn't need to be a recording artist to get invited. Just like Pelosi, figures such as Calvin Klein, Caitlyn Jenner and Shaquille O'Neal were also spotted at the gathering.\",\n",
       " 'Musical Producer Clive Davis delivering a speech at the pre-Grammys party | Photo: Getty Images']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#previewing first 10 lines to check on the text\n",
    "lines[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1bee05aa-4aab-40f9-b83f-1f009687f7ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#run the preprocesser that we definded to remove all unwanted characters and white space and generate the tokens\n",
    "for line in lines:\n",
    "    vec.append(preprocess_text(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "76be542c-3f69-439c-84bc-7b7bf59a2fc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['nancy',\n",
       "  'pelosi',\n",
       "  'receives',\n",
       "  'a',\n",
       "  'standing',\n",
       "  'ovation',\n",
       "  'from',\n",
       "  'audience',\n",
       "  'at',\n",
       "  'clive',\n",
       "  'davis',\n",
       "  '’',\n",
       "  'party'],\n",
       " ['nancy',\n",
       "  'pelosi',\n",
       "  'recently',\n",
       "  'attended',\n",
       "  'clive',\n",
       "  'davis',\n",
       "  'annual',\n",
       "  'pregrammys',\n",
       "  'party',\n",
       "  'where',\n",
       "  'she',\n",
       "  'received',\n",
       "  'a',\n",
       "  'standing',\n",
       "  'ovation']]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#previewing first 2 lines tokens\n",
    "vec[:2]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "67ec0101",
   "metadata": {},
   "source": [
    "### Defining the model\n",
    "\n",
    "The Word2Vec model require few parameters, the first is our text data tokenized,\n",
    "min_count: ignore any token that has a frequency lower than this count\n",
    "window: the distence between the predicted word and neighboring words in the text\n",
    "workers: number of parallel processes to run for this model\n",
    "\n",
    "for our case the parameters we went for are, min count of 1, and a window of 20 neighbering words, and 4 workers, and by default the method it will use is Continuous Bag-of-Words (CBOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d9af1ec-a293-4a5e-882b-5473dbfe9a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(vec, min_count=1, window=20, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9bff05ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the model for later use\n",
    "model.save(\"./word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "79383bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# after saving the model it can be loaded with this line\n",
    "model = Word2Vec.load(\"./word2vec.model\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4e1758b5",
   "metadata": {},
   "source": [
    "# testing the model\n",
    "\n",
    "We will run some tests to see if the model could get similar words to some examples and also do some arithmatic on the words to see if it works too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "333a6bbc-bbba-4bfd-bba0-6a7d1928ed58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.7477008700370789),\n",
       " ('king', 0.6856210827827454),\n",
       " ('princess', 0.6503725051879883),\n",
       " ('crown', 0.646379292011261),\n",
       " ('monarch', 0.6389058828353882)]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = model.wv[\"king\"]-model.wv[\"man\"]+model.wv[\"woman\"]\n",
    "model.wv.most_similar(vec, topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "531cdff3-d23f-431b-8712-36a24b2e4766",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('his', 0.8368909955024719),\n",
       " ('her', 0.7781476378440857),\n",
       " ('him', 0.7210463285446167),\n",
       " ('he', 0.6017439961433411),\n",
       " ('nonito', 0.5735311508178711)]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = model.wv[\"her\"]-model.wv[\"woman\"]+model.wv[\"man\"]\n",
    "model.wv.most_similar(vec, topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "553deaf2-5860-48d8-9f8d-2097c2e1d82b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lady', 0.7298709750175476),\n",
       " ('girl', 0.7199440002441406),\n",
       " ('man', 0.7185112237930298),\n",
       " ('womans', 0.6461623311042786),\n",
       " ('person', 0.6370121836662292)]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"woman\", topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a26e23f0-9463-4f8d-8031-c66e3acc7f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('aleksandr', 0.8017778992652893),\n",
       " ('mariya', 0.7633758187294006),\n",
       " ('ronaldo', 0.6185779571533203),\n",
       " ('golfers', 0.5623908638954163),\n",
       " ('cristiano', 0.5520051121711731)]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"messi\", topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a96ca7cc-d157-48d0-895e-a7f7ffbfc677",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('feline', 0.8932287693023682),\n",
       " ('kitty', 0.8868649005889893),\n",
       " ('kitten', 0.860683023929596),\n",
       " ('dog', 0.8262585997581482),\n",
       " ('puppy', 0.8212113976478577)]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"cat\", topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "9a3b90b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pup', 0.9232812523841858),\n",
       " ('puppy', 0.9159420728683472),\n",
       " ('pooch', 0.8976006507873535),\n",
       " ('cat', 0.8262584805488586),\n",
       " ('canine', 0.8040332198143005)]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"dog\", topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "aed0eaf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('mother', 0.8382915258407593),\n",
       " ('mom', 0.7471005916595459),\n",
       " ('she', 0.7199749946594238),\n",
       " ('daughter', 0.7157238721847534),\n",
       " ('grandmother', 0.7026622891426086)]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar([\"father\", \"her\"], topn=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
